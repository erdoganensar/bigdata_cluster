FROM ensaradaletai/bigdata_hadoop:3.3.4

USER root

RUN apt-get install -y python3 python3-pip python-is-python3
RUN apt-get install -y vim nano curl

# get sources
RUN mkdir /usr/share/scala
#RUN wget https://downloads.lightbend.com/scala/2.13.12/scala-2.13.12.tgz -P /tmp/
#RUN tar -xzf /tmp/scala-2.13.12.tgz -C /tmp/
#RUN mv /tmp/scala-2.13.12/* /usr/share/scala/
COPY ./scala-2.13.12/ /usr/share/scala/
#RUN rm -rf /tmp/scala-2.13.12 /tmp/scala-2.13.12.tgz
RUN cp /usr/share/scala/bin/* /usr/bin/

#RUN wget https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-without-hadoop.tgz -P /home/hadoop/
#RUN tar -xzf /home/hadoop/spark-3.3.1-bin-without-hadoop.tgz -C /home/hadoop/
#RUN mv /home/hadoop/spark-2.4.0-bin-without-hadoop /home/hadoop/spark
RUN mkdir -p /home/hadoo/spark
COPY ./spark-3.3.1-bin-without-hadoop /home/hadoop/spark
#RUN rm /home/hadoop/spark-2.4.0-bin-without-hadoop.tgz

ADD configs/postgresql-42.3.2.jar /home/hadoop/spark/jars/postgresql-42.3.2.jar

RUN mkdir /home/hadoop/spark/logs
RUN chown hadoop -R /home/hadoop/spark/logs

# set environment variables
ENV SCALA_HOME /usr/share/scala
ENV SPARK_HOME /home/hadoop/spark
ENV SPARK_LOG_DIR /home/hadoop/spark/logs
ENV PYTHONHASHSEED=1
ENV PYSPARK_PYTHON=python3
ENV SPARK_CONF_DIR=$SPARK_HOME/conf
# ENV SPARK_DIST_CLASSPATH $(hadoop classpath) does not work
RUN export SPARK_DIST_CLASSPATH=$(hadoop classpath)
ENV PATH $SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
RUN mv /home/hadoop/spark/conf/spark-env.sh.template /home/hadoop/spark/conf/spark-env.sh
RUN echo "export SPARK_DIST_CLASSPATH=$(hadoop classpath)" >> /home/hadoop/spark/conf/spark-env.sh
RUN echo "export SPARK_LOG_DIR=/home/hadoop/spark/logs" >> /home/hadoop/spark/conf/spark-env.sh
RUN mv /home/hadoop/spark/conf/spark-defaults.conf.template /home/hadoop/spark/conf/spark-defaults.conf
RUN echo "spark.eventLog.dir file:/home/hadoop/spark/logs" >> /home/hadoop/spark/conf/spark-defaults.conf
RUN echo "spark.history.fs.logDirectory file:/home/hadoop/spark/logs" >> /home/hadoop/spark/conf/spark-defaults.conf
ADD configs/log4j.properties /home/hadoop/spark/conf/log4j.properties
ADD configs/workers /home/hadoop/spark/conf/slaves
RUN chown hadoop -R /home/hadoop/spark